{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supress unnecessary warnings so that presentation looks clean\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# https://www.kaggle.com/sharmasanthosh/exploratory-study-on-ml-algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_train = pd.read_csv(\"train.csv\")\n",
    "dataset_test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "ID_test = dataset_test['id']\n",
    "\n",
    "# Dropping unnecessary columns\n",
    "\n",
    "dataset_test.drop('id', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "#Display the first five rows to get a feel of the data\n",
    "print(dataset_train.head(5))\n",
    "\n",
    "#Learning : cat1 to cat116 contain alphabets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of the dataframe\n",
    "\n",
    "print(dataset_train.shape) \n",
    "\n",
    "# We can see that there are 188318 instances/observations having 132 attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping unnecessary column 'id' in the training set since it just has serial numbers. Not useful\n",
    "# in the prediction process.\n",
    "\n",
    "dataset_train = dataset_train.iloc[:, 1:] \n",
    "\n",
    "\n",
    "\n",
    "# We dont need index 0. Just all the remaining indexes.\n",
    "\n",
    "# Statistical description\n",
    "\n",
    "print(dataset_train.describe())\n",
    "\n",
    "# Learning :\n",
    "# No attribute in continuous columns is missing as count is 188318 for all, all rows can be used\n",
    "# No negative values are present. Tests such as chi2 can be used.\n",
    "# Statistics not displayed for categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skewness of the distribution\n",
    "\n",
    "print(dataset_train.skew())\n",
    "\n",
    "# Values close to 0 show less skew.\n",
    "# loss shows the highest skew. Let us visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will visualize all the continuous attributes using Violin Plot - a combination of box and density plots\n",
    "\n",
    "# Range of features considered\n",
    "split = 116\n",
    "\n",
    "# Number of features considered\n",
    "size = 15\n",
    "\n",
    "# Creating a dataframe with only continuous features\n",
    "data = dataset_train.iloc[:, split:]\n",
    "# print(data)\n",
    "print(data.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the names of all the columns\n",
    "cols=data.columns \n",
    "\n",
    "# Plot violin for all attributes in a 7x2 grid\n",
    "n_cols = 2\n",
    "n_rows = 7\n",
    "\n",
    "for i in range(n_rows):\n",
    "    fg, ax = plt.subplots(nrows=1,ncols=n_cols,figsize=(12,8))\n",
    "    for j in range(n_cols):\n",
    "        sns.violinplot(y=cols[i*n_cols+j], data=dataset_train, ax=ax[j])\n",
    "        \n",
    "\n",
    "# Learning:        \n",
    "#cont1 has many values close to 0.5\n",
    "#cont2 has a pattern where there a several spikes at specific points\n",
    "#cont5 has many values near 0.3\n",
    "#cont14 has a distinct pattern. 0.22 and 0.82 have a lot of concentration\n",
    "#loss distribution must be converted to normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA TRANSFORMATION \n",
    "# ---> Skew Correction\n",
    "\n",
    "\n",
    "#log1p function applies log(1+x) to all elements of the column\n",
    "dataset_train[\"loss\"] = np.log1p(dataset_train[\"loss\"])\n",
    "#visualize the transformed column\n",
    "sns.violinplot(data=dataset_train,y=\"loss\")  \n",
    "plt.show()\n",
    "\n",
    "#Plot shows that skew is corrected to a large extent\n",
    "print(dataset_train[\"loss\"].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA INTERACTION\n",
    "# ---> Correlation\n",
    "\n",
    "# Correlation tells relation between two attributes.\n",
    "# Correlation requires continuous data. Therefore, we can ignore categorical data.\n",
    "\n",
    "# Calculating Pearson coefficient for all combinations\n",
    "\n",
    "data_corr = data.corr()\n",
    "print(data_corr)\n",
    "\n",
    "# Setting the threshold to select only highly correlated attributes\n",
    "\n",
    "threshold = 0.5\n",
    "\n",
    "# List of pairs along with correlation above threshold\n",
    "\n",
    "corr_list = []\n",
    "\n",
    "print(\"##########################\")\n",
    "print(data_corr.iloc[0,1])\n",
    "\n",
    "# Searching for the highly correlated pairs\n",
    "\n",
    "for i in range(0, size): #for \"size\" features\n",
    "    for j in range(i+1, size):\n",
    "        if (data_corr.iloc[i,j] >= threshold and data_corr.iloc[i,j] < 1) or (data_corr.iloc[i,j] < 0 and data_corr.iloc[i,j] <= -threshold):\n",
    "            corr_list.append([data_corr.iloc[i,j],i,j]) # stores coeffient and appropriate column indexes\n",
    "\n",
    "            \n",
    "# Sorting to show higher ones first            \n",
    "\n",
    "s_corr_list = sorted(corr_list,key=lambda x: -abs(x[0])) # See key function, https://docs.python.org/3/howto/sorting.html\n",
    "\n",
    "\n",
    "print(\"##########################\")\n",
    "\n",
    "# Printing coefficients and column names\n",
    "\n",
    "for v, i, j in s_corr_list:\n",
    "    print(\"%s and %s = %.2f\" % (cols[i],cols[j],v))\n",
    "    \n",
    "# LEARNING\n",
    "\n",
    "# We see there is a strong correlation between the following pairs: \n",
    "# This represents an opportunity to reduce the feature set through transformations such as PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of all the highly correlated pairs\n",
    "for v, i, j in s_corr_list:\n",
    "    sns.pairplot(dataset_train, size=6, x_vars=cols[i], y_vars=cols[j])\n",
    "    plt.show\n",
    "\n",
    "# cont11 and cont12 give an almost perfect linear pattern\n",
    "# cont1 and cont9  give an almost perfect linear pattern\n",
    "# cont6 and cont10 also show a very good combination\n",
    "\n",
    "#therefore one of these can be removed from each pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA VISUALIZATION\n",
    "# ---> Categorical attributes\n",
    "\n",
    "# Names of all the categorical columns\n",
    "\n",
    "cols = dataset_train.columns\n",
    "\n",
    "# Plot count plot for all attributes in a 29x4 (116 in total) grid\n",
    "\n",
    "\n",
    "n_rows = 29\n",
    "n_cols = 4\n",
    "\n",
    "for i in range(n_rows):\n",
    "    fg, ax = plt.subplots(nrows=1,ncols=n_cols,figsize=(16,8))\n",
    "    for j in range(n_cols):\n",
    "        sns.countplot(x=cols[i*n_cols+j], data=dataset_train, ax=ax[j])\n",
    "       \n",
    "    \n",
    "# LEARNING\n",
    "# cat1 to cat72 have only two labels A and B. \n",
    "# In most of the cases, B has very few entries.\n",
    "# cat73 to cat 108 have more than two labels\n",
    "# cat109 to cat116 have many labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_train.iloc[2:3, :-15].values) # debugging, manually writing to see correct index of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# DATA PREPARATION\n",
    "\n",
    "# Turning cat1 to cat116 into numerical data.\n",
    "# One-hot encoding converts an attribute to a binary vector.\n",
    "\n",
    "# Variable to store the list of variables for an attribute in the train and test set\n",
    "\n",
    "labels = []\n",
    "\n",
    "# Making sure we account for all of the unique variables that show up in both the training and test set provided. \n",
    "# For instance, this ensures we dont run into any unforeseen variables when going from the training set to test set.\n",
    "for i in range(0, split):\n",
    "    train = dataset_train[cols[i]].unique()\n",
    "    test = dataset_test[cols[i]].unique()\n",
    "    labels.append(list(set(train) | set(test))) #note the OR operator!\n",
    "\n",
    "print(\"labels %s\" % labels)\n",
    "    \n",
    "#del dataset_test\n",
    "\n",
    "\n",
    "# Importing OneHotEncoder\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "#One hot encode all categorical attributes\n",
    "cats = []\n",
    "for i in range(0, split):\n",
    "    #Label encode\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(labels[i])\n",
    "    feature = label_encoder.transform(dataset_train.iloc[:,i])\n",
    "    feature = feature.reshape(dataset_train.shape[0], 1)\n",
    "    #One hot encode\n",
    "    onehot_encoder = OneHotEncoder(sparse=False,n_values=len(labels[i]))\n",
    "    feature = onehot_encoder.fit_transform(feature)\n",
    "    cats.append(feature)\n",
    "\n",
    "\n",
    "    \n",
    "print(\"################\")\n",
    "print(\"List of 1D array of cats--> %s\" % cats)\n",
    "print(\"################\")\n",
    "# Make a 2D array from a list of 1D arrays\n",
    "encoded_cats = np.column_stack(cats)\n",
    "print(\"2D array of cats--> %s\" % encoded_cats)\n",
    "print(\"################\")\n",
    "print(encoded_cats.shape)\n",
    "print(\"################\")\n",
    "\n",
    "#Concatenate encoded attributes with continuous attributes\n",
    "dataset_train_encoded = np.concatenate((encoded_cats,dataset_train.iloc[:,split:].values),axis=1)\n",
    "del cats\n",
    "del dataset_train\n",
    "del encoded_cats\n",
    "\n",
    "\n",
    "# Print the shape of the encoded data\n",
    "print(dataset_train_encoded.shape)\n",
    "print(\"################\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA PREPARATION\n",
    "# ---> Splitting the data into train and test/valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the number of rows and columns\n",
    "\n",
    "r, c = dataset_train_encoded.shape\n",
    "\n",
    "\n",
    "# Creating an array which has indexes of columns\n",
    "i_cols = []\n",
    "\n",
    "for i in range(0, c-1):\n",
    "    i_cols.append(i)\n",
    "\n",
    "# Y is the target column, X has the rest\n",
    "X = dataset_train_encoded[:, 0:(c-1)]\n",
    "Y = dataset_train_encoded[:, (c-1)]\n",
    "\n",
    "del dataset_train_encoded\n",
    "\n",
    "\n",
    "# Validation chunk size\n",
    "val_size = 0.1\n",
    "\n",
    "# Using a common seed in all experiments so that same chunk is used for validation\n",
    "seed = 0\n",
    "\n",
    "# Splitting the data \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=val_size, random_state=seed)\n",
    "\n",
    "del X\n",
    "del Y\n",
    "\n",
    "\n",
    "# All features\n",
    "X_all = []\n",
    "\n",
    "\n",
    "# List of combinations\n",
    "comb = []\n",
    "\n",
    "# Dictionary to store the Mean Absolute Error for all algorithms\n",
    "mae = []\n",
    "\n",
    "\n",
    "#Scoring parameter\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "#Add this version of X to the list\n",
    "n = \"All\"\n",
    "\n",
    "#X_all.append([n, X_train,X_val,i_cols])\n",
    "X_all.append([n, i_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_all)     # all the columns along with dummy vars\n",
    "print(type(X_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# LINEAR REGRESSION (Linear Algo)\n",
    "\n",
    "\n",
    "\n",
    "# Fitting Linear Regression to the dataset\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression(n_jobs=-1) # using all processors\n",
    "algo = \"LR\"\n",
    "\n",
    "# Accuracy of the model using all features\n",
    "for name, i_cols_list in X_all:\n",
    "    print(name)\n",
    "    lin_reg.fit(X_train[:, i_cols_list], y_train) #fitting all features to the target column\n",
    "    result = mean_absolute_error(np.expm1(y_test), np.expm1(lin_reg.predict(X_test[:,i_cols_list])))\n",
    "    mae.append(result)\n",
    "    print(name + \" %s\" % result)\n",
    "\n",
    "comb.append(algo)\n",
    "print(comb)\n",
    "\n",
    "'''\n",
    "MODEL OUTPUT: \n",
    "\n",
    "All 1276.0276564234468\n",
    "['LR']\n",
    "'''    \n",
    "\n",
    "# #Plot the MAE of all combinations\n",
    "# fig, ax = plt.subplots()\n",
    "# plt.plot(mae)\n",
    "# #Set the tick names to names of combinations\n",
    "# ax.set_xticks(range(len(comb)))\n",
    "# ax.set_xticklabels(comb,rotation='vertical')\n",
    "# #Plot the accuracy for all combinations\n",
    "# plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN (Non-linear Algo)\n",
    "\n",
    "\n",
    "# Evaluation of various combinations of KNN\n",
    "\n",
    "# Fitting Classifier to the Training set\n",
    "from sklearn.neighbors import KNeighborsRegressor     \n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html\n",
    "\n",
    "\n",
    "\n",
    "# Add the N value to the below list if you want to run the algo\n",
    "\n",
    "n_list = np.array([]) #note, when the list is empty, the algo doesnt run\n",
    "'''\n",
    "With n_list = np.array([5])\n",
    "\n",
    "All 1434.788795356784\n",
    "['LR', 'KNN 5']\n",
    "'''\n",
    "\n",
    "'''\n",
    "With n_list = np.array([2])\n",
    "\n",
    "All 1526.7442802258656\n",
    "['LR', 'KNN 2']\n",
    "'''\n",
    "\n",
    "# we can use multiple values into n_list if we want to search for the optimal n_neighbors. However, xgboost is usally the best for parameter tuning.\n",
    "for n_neighbors in n_list:\n",
    "    # Setting the base model\n",
    "    regressor = KNeighborsRegressor(n_neighbors=n_neighbors,n_jobs=-1)\n",
    "    \n",
    "    algo = \"KNN\"\n",
    "\n",
    "    #Accuracy of the model using all features\n",
    "    for name, i_cols_list in X_all:\n",
    "        regressor.fit(X_train[:, i_cols_list], y_train) #fitting all features to the target column\n",
    "        result = mean_absolute_error(np.expm1(y_test), np.expm1(regressor.predict(X_test[:,i_cols_list])))\n",
    "        mae.append(result)\n",
    "        print(name + \" %s\" % result)\n",
    "    comb.append(algo + \" %s\" % n_neighbors)\n",
    "    \n",
    "print(comb)\n",
    "\n",
    "\n",
    "# since we know the outcome, we can skip the algorithm and append the result\n",
    "if (len(n_list)==0):\n",
    "    mae.append(1527)\n",
    "    comb.append(\"KNN\" + \" %s\" % 2 )\n",
    "\n",
    "\n",
    "\n",
    "##Set figure size, this figure compares mae for all of the algorithms ran\n",
    "\n",
    "#plt.rc(\"figure\", figsize=(25, 10))\n",
    "\n",
    "##Plot the MAE of all combinations\n",
    "#fig, ax = plt.subplots()\n",
    "#plt.plot(mae)\n",
    "##Set the tick names to names of combinations\n",
    "#ax.set_xticks(range(len(comb)))\n",
    "#ax.set_xticklabels(comb,rotation='vertical')\n",
    "##Plot the accuracy for all combinations\n",
    "#plt.show()    \n",
    "\n",
    "#Very high computation time\n",
    "#Best estimated performance is 1745 for n=1\n",
    "\n",
    "\n",
    "\n",
    "# LEARNING:\n",
    "# KNN 5 performed the best. Lowest MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CART (Non-linear Algo)\n",
    "\n",
    "\n",
    " #Evaluation of various combinations of CART\n",
    "\n",
    "#Import the library\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "#Add the max_depth value to the below list if you want to run the algo\n",
    "d_list = np.array([])\n",
    "\n",
    "for max_depth in d_list:\n",
    "    #Set the base model\n",
    "    model = DecisionTreeRegressor(max_depth=max_depth,random_state=seed)\n",
    "    \n",
    "    algo = \"CART\"\n",
    "\n",
    "    #Accuracy of the model using all features\n",
    "    for name,i_cols_list in X_all:\n",
    "        model.fit(X_train[:,i_cols_list],Y_train)\n",
    "        result = mean_absolute_error(np.expm1(Y_val), np.expm1(model.predict(X_val[:,i_cols_list])))\n",
    "        mae.append(result)\n",
    "        print(name + \" %s\" % result)\n",
    "        \n",
    "    comb.append(algo + \" %s\" % max_depth )\n",
    "\n",
    "    \n",
    "# since we know the outcome, we can skip the algorithm and append the result\n",
    "if (len(d_list)==0):\n",
    "    mae.append(1741)\n",
    "    comb.append(\"CART\" + \" %s\" % 5 )    \n",
    "    \n",
    "    \n",
    "    \n",
    "##Set figure size\n",
    "#plt.rc(\"figure\", figsize=(25, 10))\n",
    "\n",
    "##Plot the MAE of all combinations\n",
    "#fig, ax = plt.subplots()\n",
    "#plt.plot(mae)\n",
    "##Set the tick names to names of combinations\n",
    "#ax.set_xticks(range(len(comb)))\n",
    "#ax.set_xticklabels(comb,rotation='vertical')\n",
    "##Plot the accuracy for all combinations\n",
    "#plt.show()    \n",
    "\n",
    "#High computation time\n",
    "#Best estimated performance is 1741 for depth=5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM (Non-linear Algo)\n",
    "\n",
    "#Import the library\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "#Add the C value to the below list if you want to run the algo\n",
    "c_list = np.array([])\n",
    "\n",
    "for C in c_list:\n",
    "    #Set the base model\n",
    "    model = SVR(C=C)\n",
    "    \n",
    "    algo = \"SVM\"\n",
    "\n",
    "    #Accuracy of the model using all features\n",
    "    for name,i_cols_list in X_all:\n",
    "        model.fit(X_train[:,i_cols_list],Y_train)\n",
    "        result = mean_absolute_error(np.expm1(Y_val), np.expm1(model.predict(X_val[:,i_cols_list])))\n",
    "        mae.append(result)\n",
    "        print(name + \" %s\" % result)\n",
    "        \n",
    "    comb.append(algo + \" %s\" % C )\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "##Set figure size\n",
    "#plt.rc(\"figure\", figsize=(25, 10))\n",
    "\n",
    "##Plot the MAE of all combinations\n",
    "#fig, ax = plt.subplots()\n",
    "#plt.plot(mae)\n",
    "##Set the tick names to names of combinations\n",
    "#ax.set_xticks(range(len(comb)))\n",
    "#ax.set_xticklabels(comb,rotation='vertical')\n",
    "##Plot the accuracy for all combinations\n",
    "#plt.show()    \n",
    "\n",
    "#very very high computation time, not running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagged Decision Trees (Bagging)\n",
    "\n",
    "\n",
    "#Evaluation of various combinations of Bagged Decision Trees\n",
    "\n",
    "\n",
    "\n",
    "#Import the library\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "#from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "#Add the n_estimators value to the below list if you want to run the algo\n",
    "n_list = np.array([])\n",
    "\n",
    "for n_estimators in n_list:\n",
    "    #Setting the base model\n",
    "    model = BaggingRegressor(n_jobs=-1,n_estimators=n_estimators)\n",
    "    \n",
    "    algo = \"Bag\"\n",
    "\n",
    "    #Accuracy of the model using all features\n",
    "    for name,i_cols_list in X_all:\n",
    "        model.fit(X_train[:,i_cols_list],Y_train)\n",
    "        result = mean_absolute_error(np.expm1(Y_val), np.expm1(model.predict(X_val[:,i_cols_list])))\n",
    "        mae.append(result)\n",
    "        print(name + \" %s\" % result)\n",
    "        \n",
    "    comb.append(algo + \" %s\" % n_estimators )\n",
    "\n",
    "    \n",
    "    \n",
    "##Set figure size\n",
    "#plt.rc(\"figure\", figsize=(25, 10))\n",
    "\n",
    "##Plot the MAE of all combinations\n",
    "#fig, ax = plt.subplots()\n",
    "#plt.plot(mae)\n",
    "##Set the tick names to names of combinations\n",
    "#ax.set_xticks(range(len(comb)))\n",
    "#ax.set_xticklabels(comb,rotation='vertical')\n",
    "##Plot the accuracy for all combinations\n",
    "#plt.show()    \n",
    "\n",
    "#very high computation time, not running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest (Bagging)\n",
    "\n",
    "\n",
    "# Evaluation of various combinations of RandomForest\n",
    "\n",
    "#Import the library\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "#Add the n_estimators value to the below list if you want to run the algo\n",
    "n_list = np.array([])\n",
    "\n",
    "for n_estimators in n_list:\n",
    "    #Set the base model\n",
    "    model = RandomForestRegressor(n_jobs=-1,n_estimators=n_estimators,random_state=seed)\n",
    "    \n",
    "    algo = \"RF\"\n",
    "\n",
    "    #Accuracy of the model using all features\n",
    "    for name,i_cols_list in X_all:\n",
    "        model.fit(X_train[:,i_cols_list],Y_train)\n",
    "        result = mean_absolute_error(np.expm1(Y_val), np.expm1(model.predict(X_val[:,i_cols_list])))\n",
    "        mae.append(result)\n",
    "        print(name + \" %s\" % result)\n",
    "        \n",
    "    comb.append(algo + \" %s\" % n_estimators )\n",
    "\n",
    "    \n",
    "# since we know the outcome, we can skip the algorithm and append the result\n",
    "if (len(n_list)==0):\n",
    "    mae.append(1213)\n",
    "    comb.append(\"RF\" + \" %s\" % 50 )    \n",
    "    \n",
    "##Set figure size\n",
    "#plt.rc(\"figure\", figsize=(25, 10))\n",
    "\n",
    "##Plot the MAE of all combinations\n",
    "#fig, ax = plt.subplots()\n",
    "#plt.plot(mae)\n",
    "##Set the tick names to names of combinations\n",
    "#ax.set_xticks(range(len(comb)))\n",
    "#ax.set_xticklabels(comb,rotation='vertical')\n",
    "##Plot the accuracy for all combinations\n",
    "#plt.show()    \n",
    "\n",
    "#Best estimated performance is 1213 when the number of estimators is 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra Trees (Bagging)\n",
    "\n",
    "\n",
    "#Evaluation of various combinations of ExtraTrees\n",
    "\n",
    "#Import the library\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "\n",
    "#Add the n_estimators value to the below list if you want to run the algo\n",
    "n_list = np.array([])\n",
    "\n",
    "for n_estimators in n_list:\n",
    "    #Set the base model\n",
    "    model = ExtraTreesRegressor(n_jobs=-1,n_estimators=n_estimators,random_state=seed)\n",
    "    \n",
    "    algo = \"ET\"\n",
    "\n",
    "    #Accuracy of the model using all features\n",
    "    for name,i_cols_list in X_all:\n",
    "        model.fit(X_train[:,i_cols_list],Y_train)\n",
    "        result = mean_absolute_error(np.expm1(Y_val), np.expm1(model.predict(X_val[:,i_cols_list])))\n",
    "        mae.append(result)\n",
    "        print(name + \" %s\" % result)\n",
    "        \n",
    "    comb.append(algo + \" %s\" % n_estimators )\n",
    "\n",
    "    \n",
    "    \n",
    "# since we know the outcome, we can skip the algorithm and append the result\n",
    "if (len(n_list)==0):\n",
    "    mae.append(1254)\n",
    "    comb.append(\"ET\" + \" %s\" % 100 )    \n",
    "    \n",
    "    \n",
    "\n",
    "##Set figure size\n",
    "#plt.rc(\"figure\", figsize=(25, 10))\n",
    "\n",
    "##Plot the MAE of all combinations\n",
    "#fig, ax = plt.subplots()\n",
    "#plt.plot(mae)\n",
    "##Set the tick names to names of combinations\n",
    "#ax.set_xticks(range(len(comb)))\n",
    "#ax.set_xticklabels(comb,rotation='vertical')\n",
    "##Plot the accuracy for all combinations\n",
    "#plt.show()    \n",
    "\n",
    "#Best estimated performance is 1254 for 100 estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "#Evaluation of various combinations of AdaBoost\n",
    "\n",
    "#Import the library\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "#Add the n_estimators value to the below list if you want to run the algo\n",
    "n_list = np.array([])\n",
    "\n",
    "for n_estimators in n_list:\n",
    "    #Set the base model\n",
    "    model = AdaBoostRegressor(n_estimators=n_estimators,random_state=seed)\n",
    "    \n",
    "    algo = \"Ada\"\n",
    "\n",
    "    #Accuracy of the model using all features\n",
    "    for name,i_cols_list in X_all:\n",
    "        model.fit(X_train[:,i_cols_list],Y_train)\n",
    "        result = mean_absolute_error(np.expm1(Y_val), np.expm1(model.predict(X_val[:,i_cols_list])))\n",
    "        mae.append(result)\n",
    "        print(name + \" %s\" % result)\n",
    "        \n",
    "    comb.append(algo + \" %s\" % n_estimators )\n",
    "    \n",
    "    \n",
    "# since we know the outcome, we can skip the algorithm and append the result\n",
    "if (len(n_list)==0):\n",
    "    mae.append(1678)\n",
    "    comb.append(\"Ada\" + \" %s\" % 100 )    \n",
    "    \n",
    "##Set figure size\n",
    "#plt.rc(\"figure\", figsize=(25, 10))\n",
    "\n",
    "##Plot the MAE of all combinations\n",
    "#fig, ax = plt.subplots()\n",
    "#plt.plot(mae)\n",
    "##Set the tick names to names of combinations\n",
    "#ax.set_xticks(range(len(comb)))\n",
    "#ax.set_xticklabels(comb,rotation='vertical')\n",
    "##Plot the accuracy for all combinations\n",
    "#plt.show()    \n",
    "\n",
    "#Best estimated performance is 1678 with n=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic Gradient Boosting (Boosting)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Evaluation of various combinations of SGB\n",
    "\n",
    "#Import the library\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "#Add the n_estimators value to the below list if you want to run the algo\n",
    "n_list = np.array([])\n",
    "\n",
    "for n_estimators in n_list:\n",
    "    #Set the base model\n",
    "    model = GradientBoostingRegressor(n_estimators=n_estimators,random_state=seed)\n",
    "    \n",
    "    algo = \"SGB\"\n",
    "\n",
    "    #Accuracy of the model using all features\n",
    "    for name,i_cols_list in X_all:\n",
    "        model.fit(X_train[:,i_cols_list],Y_train)\n",
    "        result = mean_absolute_error(np.expm1(Y_val), np.expm1(model.predict(X_val[:,i_cols_list])))\n",
    "        mae.append(result)\n",
    "        print(name + \" %s\" % result)\n",
    "        \n",
    "    comb.append(algo + \" %s\" % n_estimators )\n",
    "\n",
    "# since we know the outcome, we can skip the algorithm and append the result\n",
    "if (len(n_list)==0):\n",
    "    mae.append(1278)\n",
    "    comb.append(\"SGB\" + \" %s\" % 50 )    \n",
    "    \n",
    "##Set figure size\n",
    "#plt.rc(\"figure\", figsize=(25, 10))\n",
    "\n",
    "##Plot the MAE of all combinations\n",
    "#fig, ax = plt.subplots()\n",
    "#plt.plot(mae)\n",
    "##Set the tick names to names of combinations\n",
    "#ax.set_xticks(range(len(comb)))\n",
    "#ax.set_xticklabels(comb,rotation='vertical')\n",
    "##Plot the accuracy for all combinations\n",
    "#plt.show()    \n",
    "\n",
    "#Best estimated performance is ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #XGBoost\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#Evaluation of various combinations of XGB\n",
    "\n",
    "#Import the library\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "#Add the n_estimators value to the below list if you want to run the algo\n",
    "n_list = np.array([])\n",
    "\n",
    "for n_estimators in n_list:\n",
    "    #Set the base model\n",
    "    model = XGBRegressor(n_estimators=n_estimators,seed=seed)\n",
    "    \n",
    "    algo = \"XGB\"\n",
    "\n",
    "    #Accuracy of the model using all features\n",
    "    for name,i_cols_list in X_all:\n",
    "        model.fit(X_train[:,i_cols_list],Y_train)\n",
    "        result = mean_absolute_error(np.expm1(Y_val), np.expm1(model.predict(X_val[:,i_cols_list])))\n",
    "        mae.append(result)\n",
    "        print(name + \" %s\" % result)\n",
    "        \n",
    "    comb.append(algo + \" %s\" % n_estimators )\n",
    "\n",
    "# since we know the outcome, we can skip the algorithm and append the result\n",
    "if (len(n_list)==0):\n",
    "    mae.append(1169)\n",
    "    comb.append(\"XGB\" + \" %s\" % 1000 )    \n",
    "    \n",
    "##Set figure size\n",
    "#plt.rc(\"figure\", figsize=(25, 10))\n",
    "\n",
    "##Plot the MAE of all combinations\n",
    "#fig, ax = plt.subplots()\n",
    "#plt.plot(mae)\n",
    "##Set the tick names to names of combinations\n",
    "#ax.set_xticks(range(len(comb)))\n",
    "#ax.set_xticklabels(comb,rotation='vertical')\n",
    "##Plot the accuracy for all combinations\n",
    "#plt.show()    \n",
    "\n",
    "#Best estimated performance is 1169 with n=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MLP (Deep Learning)\n",
    "\n",
    "\n",
    "#Evaluation of various combinations of multi-layer perceptrons\n",
    "\n",
    "#Import libraries for deep learning\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# define baseline model\n",
    "def baseline(v):\n",
    "     # create model\n",
    "     model = Sequential()\n",
    "     model.add(Dense(v*(c-1), input_dim=v*(c-1), init='normal', activation='relu'))\n",
    "     model.add(Dense(1, init='normal'))\n",
    "     # Compile model\n",
    "     model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "     return model\n",
    "\n",
    "# define smaller model\n",
    "def smaller(v):\n",
    "     # create model\n",
    "     model = Sequential()\n",
    "     model.add(Dense(v*(c-1)/2, input_dim=v*(c-1), init='normal', activation='relu'))\n",
    "     model.add(Dense(1, init='normal', activation='relu'))\n",
    "     # Compile model\n",
    "     model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "     return model\n",
    "\n",
    "# define deeper model\n",
    "def deeper(v):\n",
    " # create model\n",
    " model = Sequential()\n",
    " model.add(Dense(v*(c-1), input_dim=v*(c-1), init='normal', activation='relu'))\n",
    " model.add(Dense(v*(c-1)/2, init='normal', activation='relu'))\n",
    " model.add(Dense(1, init='normal', activation='relu'))\n",
    " # Compile model\n",
    " model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    " return model\n",
    "\n",
    "# Optimize using dropout and decay\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Dropout\n",
    "from keras.constraints import maxnorm\n",
    "\n",
    "def dropout(v):\n",
    "    #create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(v*(c-1), input_dim=v*(c-1), init='normal', activation='relu',W_constraint=maxnorm(3)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(v*(c-1)/2, init='normal', activation='relu', W_constraint=maxnorm(3)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, init='normal', activation='relu'))\n",
    "    # Compile model\n",
    "    sgd = SGD(lr=0.1,momentum=0.9,decay=0.0,nesterov=False)\n",
    "    model.compile(loss='mean_absolute_error', optimizer=sgd)\n",
    "    return model\n",
    "\n",
    "# define decay model\n",
    "def decay(v):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(v*(c-1), input_dim=v*(c-1), init='normal', activation='relu'))\n",
    "    model.add(Dense(1, init='normal', activation='relu'))\n",
    "    # Compile model\n",
    "    sgd = SGD(lr=0.1,momentum=0.8,decay=0.01,nesterov=False)\n",
    "    model.compile(loss='mean_absolute_error', optimizer=sgd)\n",
    "    return model\n",
    "\n",
    "\n",
    "est_list = []\n",
    "#uncomment the below if you want to run the algo\n",
    "#est_list = [('MLP',baseline),('smaller',smaller),('deeper',deeper),('dropout',dropout),('decay',decay)]\n",
    "\n",
    "for name, est in est_list:\n",
    " \n",
    "    algo = name\n",
    "\n",
    "    #Accuracy of the model using all features\n",
    "    for m,i_cols_list in X_all:\n",
    "        model = KerasRegressor(build_fn=est, v=1, nb_epoch=10, verbose=0)\n",
    "        model.fit(X_train[:,i_cols_list],Y_train)\n",
    "        result = mean_absolute_error(np.expm1(Y_val), np.expm1(model.predict(X_val[:,i_cols_list])))\n",
    "        mae.append(result)\n",
    "        print(name + \" %s\" % result)\n",
    "        \n",
    "    comb.append(algo )\n",
    "    \n",
    "    \n",
    "# since we know the outcome, we can skip the algorithm and append the result\n",
    "if (len(est_list)==0):\n",
    "    mae.append(1168)\n",
    "    comb.append(\"MLP\" + \" baseline\" )    \n",
    "    \n",
    "    \n",
    "    \n",
    "print(\"mae--> %s\" % mae)\n",
    "print(\"comb--> %s\" % comb)\n",
    "##Set figure size\n",
    "plt.rc(\"figure\", figsize=(25, 10))\n",
    "\n",
    "#Plot the MAE of all combinations\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(mae)\n",
    "#Set the tick names to names of combinations\n",
    "ax.set_xticks(range(len(comb)))\n",
    "ax.set_xticklabels(comb,rotation='vertical')\n",
    "#Plot the accuracy for all combinations\n",
    "plt.show()    \n",
    "\n",
    "#Best estimated performance is MLP=1168"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Since XGBRegressor is showing the best performance, we can select it as our best model. Therefore, we now need to finalize the model with all of the avialable data.\n",
    "'''\n",
    "\n",
    "\n",
    "# note, X_train and X_test are both coming from the training set CSV. axis=0 is stacking rows on top of one another.\n",
    "X = np.concatenate((X_train,X_test), axis=0) \n",
    "del X_train\n",
    "del X_test\n",
    "Y = np.concatenate((y_train,y_test),axis=0)\n",
    "del y_train\n",
    "del y_test\n",
    "\n",
    "print(\"I am here 0 - debug\")\n",
    "\n",
    "\n",
    "n_estimators = 1000\n",
    "\n",
    "#Best model definition\n",
    "best_model = XGBRegressor(n_estimators=n_estimators,seed=seed)\n",
    "print(\"I am here 0.0 - debug\")\n",
    "best_model.fit(X,Y)\n",
    "print(\"I am here 0.1 - debug\")\n",
    "del X\n",
    "del Y\n",
    "#Read test dataset\n",
    "dataset_test = pd.read_csv(\"test.csv\")\n",
    "print(\"I am here 0.2 - debug\")\n",
    "#Drop unnecessary columns\n",
    "ID = dataset_test['id']\n",
    "dataset_test.drop('id',axis=1,inplace=True)\n",
    "\n",
    "#One hot encode all categorical attributes\n",
    "cats = []\n",
    "print(\"I am here 1 - debug\")\n",
    "for i in range(0, split):\n",
    "    # label encoding\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(labels[i])\n",
    "    feature = label_encoder.transform(dataset_test.iloc[:,i])\n",
    "    feature = feature.reshape(dataset_test.shape[0], 1)\n",
    "    #One hot encoding\n",
    "    onehot_encoder = OneHotEncoder(sparse=False,n_values=len(labels[i]))\n",
    "    feature = onehot_encoder.fit_transform(feature)\n",
    "    cats.append(feature)\n",
    "\n",
    "print(\"I am here 2 - debug\")\n",
    "# Making a 2D array from a list of 1D arrays\n",
    "encoded_cats = np.column_stack(cats)\n",
    "del cats\n",
    "\n",
    "# Concatenating encoded attributes with continous attributes\n",
    "X_test = np.concatenate((encoded_cats, dataset_test.iloc[:,split:].values), axis=1)\n",
    "print(\"I am here 3 - debug\")\n",
    "del encoded_cats\n",
    "del dataset_test\n",
    "\n",
    "# Making predictions using the best model now\n",
    "predictions = np.expm1(best_model.predict(X_test))\n",
    "\n",
    "with open(\"submission.csv\", \"w\") as subfile:\n",
    "    print(\"I am here 4 - debug\")\n",
    "    subfile.write(\"id, loss\\n\") #column headers\n",
    "    for i, pred in enumerate(list(predicitions)):\n",
    "        subfile.write(\"%s,%s\\n\" % (ID[i], pred)) \n",
    "\n",
    "\n",
    "print(\"I am here 5 - debug\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
